\chapter{A Historical Review}
\label{ch2}

\section{The Role of Annotation}


As one of the most important subjects in artificial intelligence, computer vision enables computers to identify, perceive, and recognize people, places, and things, and ultimately imitate natural vision. The current state of computer vision is vulnerable to attack, unadaptable to new surroundings, and incapable of life-long learning. To match natural vision, our journey's just begun. 

\textit{Do we need annotation to develop human-like computer vision?} The necessity, formation, and quantity of annotation is fundamentally dependent on the learning objective---what do we wish the computer to learn?  An established learning objective can determine whether we should collect manual annotation and, if yes, what the type of the annotation is. For example, the learning objective of classifying 14 diseases requires the annotator to identify the types of diseases in the image; the learning objective of segmenting lung nodule requires the annotator to outline the boundary of each nodule. Defining the learning objective for specific imaging tasks is straightforward, but the learning objective for the task of matching natural vision is still inconclusive. This has led to spiraling debates on the necessity of acquiring manual annotation for developing human-like computer vision. In essence, the debates are about the learning objective of computer vision.

\subsection{Attribute Learning}

The earliest attempts to develop computer vision involved the idea that a visual concept (\eg cat) can be described and predicted by several attributes (\eg round face, chubby body, two pointy ears, and a long tail). If any object carries these preset attributes, the computer can identify cats from many images. While more advanced and sophisticated attributes arise, the underlying learning objective behind these approaches remains similar---identifying these descriptive attributes from the image. However, using these approaches, computers can make many simple mistakes, such as when (1) the objects are overlapping, (2) the object's position and shape are distorted, or (3) the object is conceptually difficult to define. The attribute-based approaches lack reliability, as countless concepts demand too much manual intervention for their definition and numerous variations that can eliminate the rule of conceptual modeling. To move away from extensive attribute engineering, researchers sought to automate feature learning for object recognition. 

\subsection{Categorical Learning}

Inspired by cognitive science and neuroscience, Drs.~Geoffrey~Hinton, Yann~LeCun, and Yoshua~Bengio developed an algorithm called deep neural networks~\citep{lecun1989backpropagation,bengio2009learning} that makes automated feature learning possible, but its strengths were not appreciated until the availability of big image datasets. At the beginning of 2007, Dr.~Fei-Fei~Li started creating a large-scale image dataset~\citep{deng2009imagenet}. She held the belief that developing reliable computer vision systems requires a lot of human annotated examples. Imagine a child's eyes as a pair of biological cameras, and they take one image about every 200 milliseconds. By age three, the child would have seen a tremendous number of real-world images. This observation promoted multiple large-scale, systematic-labeled datasets in the last few years. Deep neural networks trained on these datasets have enabled enormous advances in computer vision, leading to amazing results on some real-world tasks, such as object recognition, detection, segmentation, and image captioning. Additionally, in academic settings, deep neural networks almost always outperform alternative attribute-based approaches on benchmark tasks.

Combining large datasets, deep neural networks, and powerful computers, categorical supervised learning emerged as a new learning paradigm, where the learning objective for computers is to minimize the error between computer predictions and human labels. Here, humans play an essential role in training computers in this learning paradigm because humans must provide all categorical labels for the dataset. Although training deep neural networks using categorical supervised learning is quite effective, there are three inherent restrictions: (1) computers can only differentiate the specific categories given by humans, but not beyond; (2) computers can perform poorly on real-world images outside the dataset; and, most importantly, (3) the resulting computer vision is much less general, flexible, and adaptive than natural vision. Categories and concepts in the real world can be far more comprehensive than those given in the benchmark datasets. It is because the categories in the real world are non-orthogonal (cat and tiger vs. cat and plane), imbalanced (long-tail distribution for most classes), and exponential (classes with hierarchical sub-classes). Since a computer is unable to learn categories beyond what has been given, the annotating work can keep going on indefinitely, and the resultant computer vision would always be tied with specific categories. The categorical supervised learning paradigm is essentially the same as attribute-based learning, where categories serve as attributes to help computers understand the world. 


The major concern is not the challenge to annotate an adequate number of images but rather the fact that learning paradigms are fundamentally asymmetrical between computer vision and natural vision, in which the former is currently built upon categorical labels while the latter is developed from images without any label. Human babies and animals establish vision naturally without direct supervision---in nature, there is no dictionary of concepts available---they learn these through real-world experiences and interactions. Although the top-down categorization, based on a linguistic definition, can help develop task-specific computer vision systems, it might be unnecessary for a general-purpose visual system. To deal with the enormous complexity of natural images and obtain the rich understanding of visual scenes that the human achieves, today, we still yearn to know the underlying objective of natural vision~\citep{yuille2021deep}. 

\subsection{Representation Learning}

The dissimilarity between natural vision and current computer vision suggests alternative learning paradigms. Self-supervised learning is an interesting reflection on the general thought on learning representation in a way similar to natural vision. This learning paradigm has existed for some time, but its power historically has lagged behind the state-of-the-art categorical supervised learning. However, the recent pace of progress in self-supervised learning has increased dramatically and led to visual representation that approaches and even surpasses the representation learned from supervised categorization. It has raised hopes that self-supervised learning could indeed replace the ubiquitous categorical supervised learning in advanced computer vision going forward. Unlike categorical supervised learning, a computer does not have to learn orthogonal, balanced, and finite categories from human annotation; instead, it learns by studying the properties of real-world images. Self-supervision promises to get away from top-down categorization and enable continuous life-long learning. As highly advocated by Drs. Yann LeCun and Yoshua Bengio, ``self-supervised learning is the key to human-level intelligence.''~\citep{wiggers2020yann}

The line of research on self-supervision is more closely investigating the objective of natural vision development. As a learner interacts with the environment, one of the most common objectives is to survive---to avoid either being attacked or starving---which has led to two major research avenues in self-supervision: (1) learning a predictive model to fill in the blank and (2) learning a contrastive model to distinguish multiple views. First, to prevent being attacked or killed, a learner should develop meaningful expectations about the world, coming up with a hypothesis of the world and then verifying it. As a result, the predictive model predicts some hidden information (\eg color, future events, or contexts of an image) to perceive prior knowledge and physical properties in nature, such as the sky being blue or a running beast approaching you. Second, to ensure survival, a learner is expected to distinguish objects (\eg determining food edibility based on color, shape, texture, etc.). It should be noted that distinguishing is different from categorizing because the distinction can separate things even if they belong to the same category. Consequently, instead of categorization, the contrastive model compares images that have undergone strong data augmentation to learn image representation, which is resilient to various view changes.

\subsection{Current Limitations and Future Considerations}

In the discussion above, we have been following a similar principle to develop general-purpose computer vision: \textit{do not define anything}. While learning algorithms are continually changing as better methods are developed, one trend that is not going away is the move towards increased levels of automation. We seek for a way to let computers autonomously interact with images and capture visual representation, keeping away from manually defining attributes, categories, etc. Automated feature learning will save time, build generic models, create meaningful features, and encourage learning from diverse data sources. As of now, compared with natural vision, the current state of self-supervision is incomplete in at least three ways.

\begin{itemize}
    \item \textit{First, the choice of augmented views is supervised by humans.} Data augmentation is widely used for training both predictive and contrastive models due to its simplicity and efficiency. A predictive model restores the original images from the transformed ones through data augmentation; a contrastive model distinguishes the same image from different views generated from data augmentation. However, humans must pre-define a set of data augmentations specific to each task because some augmentations can make a task ambiguous, unsolvable, or trivial, leading to degenerate learning. Here comes several examples: cropping patches from images can occlude the target object; permutating color is mostly not applicable to grayscale images; predicting rotation angles in medical images can be trivial due to the consistent anatomical structure. Many recent works appear to automate data augmentation in self-supervised learning, one of which is to use videos rather than images. Humans learn from a sequence of meaningful images instead of a large number of non-related still images because videos naturally associate with different continuous views. Another way is to use generated images so that bottleneck features can manipulate the image context to ensure target objects' existence.
    
    \item \textit{Second, the choice of model architectures is supervised by humans.} In the existing literature, methods are generally developed to learn the weights (parameters) of a ﬁxed architecture without using labels, and these weights are evaluated by transferring to a target supervised task. In a recent study, \citet{liu2020labels} explored the possibility of using such methods to learn architecture without using labels. The neural architecture search seems to relax the manual design, but the search space heavily relies on humans. There are three challenges associated with the existing approaches. (1) The neural connection can never be found if it is not included in the original search space—the search space limits what neural architecture can be discovered. (2) The searching will terminate into a fixed architecture if it meets a local minimum. In contrast, the neural connection in human brains is dynamically evolving throughout the lifespan. (3) Vast computational resources are required for the neural architecture search, while the resultant architecture cannot guarantee superior outcomes to human-engineered architectures~\citep{isensee2021nnu}. In addition, although convolutional neural networks are currently dominant in most imaging tasks, another architecture called transformer was proven more powerful to encode long-term dependencies among data~\citep{vaswani2017attention,dosovitskiy2020image}, therefore exceeding in analyzing sequences of data such as language and video.
    
    \item \textit{Third, the choice of pretext tasks is supervised by humans.} That being said, a wide range of learning schemes with varying learning objectives are currently designed by humans, such as predicting rotation, augmentation, color, etc~\citep{jing2020self}. But the fact is, we are unsure how exactly natural vision is developed, as we are the users, not the designers. It is possible that pre-defined learning schemes, either filling in blanks or contrasting views, could dilute the true power of self-supervised learning. Given an image, human vision is developed by multi-tasking, such as depth estimation, motion prediction, orientation perception, object detection, etc. The types of these tasks are not pre-defined but driven by an underlying objective. We have given special prominence to the objective that drives a learner to develop vision because it is the learning objective that mostly makes such diverse types of tasks for us to learn, even though sometimes our supervisors (parents, teachers, primers) suggest some specific tasks for us. Instead of devising many pretext tasks, the real mission is to figure out the true objective beyond vision, which comes up with a research field called learning to learn or meta learning~\citep{lake2015human}. According to the concept of meta learning, a learner itself must be exposed to a large number of tasks and tested on their ability to learn new tasks. Thus, humans do not have to design which tasks to solve, and instead, computers make up their own games to develop computer vision.
\end{itemize}


As revealed in a historical review, it remains an open problem to construct a complete, unified learning objective of computer vision using one concise equation. In the past decades, we have made exciting progress by discovering partial learning objectives that make computers accomplish specific tasks and developing critical components that collectively simulate natural vision. We are heading towards the direction where the advancements in computer vision rely less and less on manual annotation to secure comprehensive visual knowledge from images. 
% Finally, we have outlined three future premises for promoting annotation-efficient deep learning, which places a great ending point of my incredible Ph.D. journey and opens a new chapter for future exploration.


\section{The Opportunity: Annotation-Efficient Deep Learning}
\label{ch2:challenge_opportunity:opportunity}

% {\jlred Why annotation-efficient deep learning is an opportunity?}
This section overviews three advantages that have stimulated annotation-efficient deep learning and resulted in numerous emerging subjects, including our contributions in this dissertation. 

\begin{enumerate}
    \item \textit{The continual learning capabilities of deep learning incrementally improve the algorithm through fine-tuning.} Millions of new medical images are generated in hospitals every day. With such a colossal stream of data, it is impracticable to store the data in memory and repeatedly train computers from scratch once new data becomes available. We hope computers to leverage the prior knowledge obtained from old data over time and continuously accommodate new data, like human beings. Continual learning is built on the idea that learners adaptively use new data so their knowledge sets can develop autonomously and incrementally. The continual learning ability is one of the critical benefits that deep learning could offer. Unlike conventional machine learning methods, deep learning models can be fine-tuned on top of previously learned weights that often store the memories and knowledge of old data. Specifically, we can take a set of trained weights and use it as model initialization for new data. The ability of continual learning would be much more appreciated in the scenario of the ``human-in-the-loop'' procedure, wherein human experts interact with computers to promote the development of algorithms using a continuous stream of data. An efficient ``human-in-the-loop'' procedure helps human experts quickly dismiss patients with negative results, therefore, dramatically reducing the burden of annotation. Moreover, an instant online feedback process encourages data, annotation, and model reuse, making it possible for CAD systems to self-improve via continual fine-tuning.
    
    \item \textit{The representation learning capabilities of deep learning alleviate exhaustive feature engineering for specific medical conditions.} Feature engineering manually designs features based on the texture and shape present in images, which are easier to describe and troubleshoot so humans can manipulate features on their own. However, crafting such features demands a great deal of patience, diligence, and expertise. Most hand-crafted features focus on specific medical conditions, hence greatly limiting the expressive powers and depreciating the generalization capacity. For instance, radiomics features can be beneficial in radiological imaging, but they are not adaptable to other imaging modalities, such as dermatology, histopathology, and ophthalmology. Recent deep learning methods swept away previous hand-crafted features, showing that neural networks can solve diverse tasks by automatically learning hierarchical features at multiple levels of abstraction. In networks, each layer projects the image into a particular feature space---the deeper layer generates a higher level of abstraction by extracting more complex features built on top of simpler ones. The merit of deep learning is that the varying levels of features are not manually designed by humans. For this, we call it ``representation learning'', a procedure that automatically learns visual features to represent an image. Representation learning is more efficient and repeatable than exhaustive feature engineering, saving tremendous amounts of manual work. Compared with hand-crafted features, deep features offer four advantages: (1) deep features can be dynamically computed by models during training and test stages; (2) deep features present a semantic hierarchy, varying from layer to layer; (3) deep features can be used for not only classification but also registration, localization, and segmentation; (4) deep features can be fine-tuned and adapted to different tasks and domains. Many studies have reaffirmed that automated feature learning can produce more generalizable image representation than hand-crafted features.
    
    \item \textit{The consistent and recurrent anatomy embedded in medical images empowers deep learning with a generic visual representation.} Human anatomies are intrinsically structured, exhibiting consistency in appearance, position, and layout. Medical imaging protocols focus on particular parts of the body, often generating images of great similarity and yielding an abundance of sophisticated anatomical patterns across patients. These patterns are naturally associated with comprehensive knowledge about human anatomy. Therefore, consistent and recurrent anatomy can ease the analysis of numerous critical problems and should be considered a significant advantage of medical imaging. Due to the recurring anatomy, the same body parts in different images express similar visual patterns and, therefore, can be retrieved by what is known as ``nearest neighbor search''. As a result, given a single annotated medical image, similar anatomical patterns can be found in many other images so that radiologists can track disease progress with landmark detection and lesion matching. In addition to correspondence matching, the recurrent anatomical structures in medical images are associated with rich knowledge about the human body and intrinsic structural coherence, offering great benefit and potential to foster image representation and produce more powerful source models. Consequently, one-shot or few-shot learning in various medical applications would be eventually actualized. 
\end{enumerate}


\section{Related Work \& Our Innovations}
\label{ch1:related_work}


We extensively review the related work that tackles the significant barrier of annotation sparsity by harnessing the three unique advantages, while underlining the novelty of the methodologies that we have developed.

\subsection{Acquiring Necessary Annotation}
\label{ch1:related_work:acquiring_necessary_annotations}

\subsubsection{One-time learning and continual learning}
\label{ch1:related_work:acquiring_necessary_annotation:transfer_learning_medical_imaging}

Pre-training a model on large-scale image datasets and then fine-tuning it on various target tasks has become a \textit{de facto} paradigm across many medical specialties. As summarized by~\citet{irvin2019chexpert}, to classify the common thoracic diseases on chest radiography, nearly all the leading approaches~\citep{guan2018multi,guendel2018learning,tang2018attention,ma2019multi} follow this paradigm by adopting different architectures along with their weights pre-trained from ImageNet. Other representative medical applications include identifying skin cancer from dermatologist level photographs~\citep{esteva2017dermatologist}, diagnosing Alzheimer's Disease~\citep{ding2018deep} from $^{18}$F-FDG PET of the brain, and performing effective detection of pulmonary embolism~\citep{tajbakhsh2019computer} from CTPA. 
Recent breakthrough in self-supervised pre-training~\citep{grill2020bootstrap,caron2020unsupervised,chen2020exploring}, on the other hand, has led to visual representation that approaches and possibly surpasses what was learned from ImageNet. Self-supervised pre-training has also been adopted for the medical domain, wherein \citet{zhou2019models,zhu2020rubik,feng2020parts2whole,haghighi2020learning,azizi2021big} develop generic CNNs that are directly pre-trained from medical images, mitigating the mandatory requirement of expert annotation and reducing the large domain gap between natural and medical images. Despite the immense popularity of transfer learning in medical imaging, these works exclusively employed {\em one-time fine-tuning}---simply fine-tuning a pre-trained CNN,  for only one time, with available training samples. In real-world applications, instead of training on a still dataset, experts record new samples constantly and expect the samples to be used upon their availability. Therefore, by empowering the CNN with the ability to deal with new data, continual learning is the bridge to active and open world learning~\citep{mundt2020wholistic}. Compared with the existing continual learning approaches~\citep{kading2016fine,zhou2017fine}, our newly devised learning strategy is more amenable to active fine-tuning because it focuses more on the newly annotated samples and also recognizes those misclassified ones, eliminating repeated training on easier samples in the annotated pool.


\subsubsection{Integrating active learning with deep learning}
\label{ch1:related_work:acquiring_necessary_annotation:integrating_active_learning_deep_learning}


The uncertainty and diversity are the most compelling active selection criteria, which appraise the worthiness of annotating a sample from two different aspects. Uncertainty-based criteria argue that the more uncertain a prediction is, the more value added when including the label of that sample into the training set. Sampling with least confidence~\citep{culotta2005reducing}, large entropy~\citep{dagan1995committee,mahapatra2018efficient,shao2018deep,kuo2018cost}, or margins~\citep{scheffer2001active,balcan2007margin} of the prediction has been successful in training models with fewer labels than random sampling. The limitation of uncertainty-based criteria is that some of the selected samples are prone to redundancy and outliers~\citep{sourati2019intelligent} and may not be representative enough for the data distribution as a whole. Alternatively, diversity-based criteria have the advantage of selecting a set of most representative samples, related to the annotated ones, from those in the rest of the unannotated set. The intuition is it is unnecessary to repeatedly annotate similar samples. Mutual information~\citep{li2013adaptive,gal2017deep}, Kullback-Leibler divergence~\citep{kulick2014active,mccallumzy1998employing}, Fisher information~\citep{sourati2018active,sourati2019intelligent}, K-centers and core sets~\citep{sener2017active}, calculated among either model predictions or image features, are often used to ensure the diversity. Although alleviating redundancy and outliers, a serious hurdle of diversity-based criteria is the computational complexity for a large pool of unannotated samples. We address this issue by measuring diversity over patches augmented from the same sample, making the calculation much more manageable. To exploit the benefits and potentials of the two selecting aspects, the studies of \citet{wang2018deep,ozdemir2018active,mahapatra2018efficient,shui2020deep} consider the mixture strategy of combing uncertainty and diversity explicitly. \citet{yang2017suggestive,beluch2018power,kuo2018cost} further compute the selection criteria from an ensemble of CNNs---these approaches are, however, very costly in computation, as they must train a set of models to compute their uncertainty measure based on models' disagreements. For additional active learning methods, we refer the reader to comprehensive literature reviews~\citep{tajbakhsh2020embracing,munjal2020towards,hino2020active,ren2020survey}; but these existing methods are fundamentally different from our active continual fine-tuning (ACFT) in that they all repeatedly re-trained CNNs from scratch at each step, whereas we continually fine-tune the (fine-tuned) CNN incrementally. As a result, our ACFT offers several advantages as listed in Sec.~\ref{ch3:approach_property:several_unique_properties}, and leads to dramatic annotation cost reduction and computation efficiency. Besides, we have found that there are only seven fundamental patterns in CNN predictions, as summarized in~\tablename~\ref{ch3:tab:predict_pattern}. Multiple methods may be developed to select a particular pattern: entropy, Gaussian distance, and standard deviation would seek Pattern A, while diversity, variance, and divergence look for Pattern C. We were among the first to analyze the prediction patterns in active learning and investigate the effectiveness of typical patterns rather than comparing the many methods. 


% \subsection{Medical image segmentation}
\subsection{Designing Advanced Architectures}
\label{ch1:related_work:designing_advanced_architectures}

% In the following, we review the works related to redesigned skip connections, feature aggregation, and deep supervision, which are the main components of our new architecture.

\subsubsection{Skip connections}
\label{ch1:related_work:designing_advanced_architectures:skip_connections}

Skip connections were first introduced in the seminal work of
\citet{long2015fully} where they proposed fully convolutional networks (FCN) for semantic segmentation. Shortly after, building on skip connections, \citet{ronneberger2015u} proposed U-Net architecture for semantic segmentation in medical images. The FCN and U-Net architectures, however, differ in how the decoder features are fused with the same-scale encoder features. While FCN~\citep{long2015fully} uses the summation operation for feature fusion,  U-Net~\citep{ronneberger2015u} concatenates the features followed by the application of convolutions and non-linearities. The skip connections have shown to help recover the full spatial resolution, making fully convolutional methods suitable for semantic segmentation~\citep{chaurasia2017linknet,lin2017refinenet,zhao2018icnet,tajbakhsh2020errornet}. 
Skip connections have further been used in modern neural architectures such as  residual networks~\citep{he2016deep,he2016identity} and dense networks~\citep{huang2017densely}, facilitating the gradient flow and improving the overall performance of classification networks.

\subsubsection{Aggregating multi-scale features}
\label{ch1:related_work:designing_advanced_architectures:feature_aggregation}
The exploration of aggregating hierarchical features continues to be a popular subject of research. \citet{fourure2017gridnet} propose GridNet, which is an encoder-decoder architecture wherein the feature maps are wired in a grid fashion, generalizing several classical segmentation architectures. Despite GridNet containing multiple streams with different resolutions, it lacks up-sampling layers between skip connections; and thus, it does not represent UNet++.
Full-resolution residual networks (FRRN)~\citep{pohlen2017full} employs a two-stream system, where full-resolution information is carried in one stream and context information in the other pooling stream. In~\citet{jiang2019multiple}, two improved versions of FRRN are proposed, \ie incremental MRRN with 28.6M parameters and dense MRRN with 25.5M parameters. 
These 2D architectures, however, have similar number of parameters to our 3D VNet++ and three times more parameters than 2D UNet++; and thus, simply extending these architectures to a 3D manner may not be amenable to the common 3D medical applications.
We would like to note that our redesigned dense skip connections are completely different from those used in MRRN, which consists of a common residual stream. Also, it is not flexible to apply the design of MRRN to other backbone encoders and meta framework such as Mask R-CNN~\citep{he2017mask}. Deep layer aggregation (DLA)~\citep{yu2018deep}, topologically equivalent to our intermediate architecture UNet+ (\figurename~\ref{ch4:fig:network_architecture}(f)), adjacently connects the same resolution features without U-Net's long skip connections. Our experimental results demonstrate that by densely connecting the layers, UNet++ achieves higher segmentation performance than UNet+/DLA (see \tableautorefname~\ref{ch4:tab:main_results}). 

\subsubsection{Introducing deep supervision}
\label{ch1:related_work:designing_advanced_architectures:deep_supervision}

He~\etal~\citep{he2016deep} suggested that the depth of network can act as a regularizer. Lee~\etal~\citep{lee2015deeply} demonstrated that deeply supervised layers can improve the learning ability of hidden layers, enforcing the intermediate layers to learn discriminative features, enabling  fast convergence and regularization of the network~\citep{dou20173d}. DenseNet~\citep{huang2017densely} performs a similar deep supervision in an implicit fashion. Deep supervision can also be used in U-Net like architectures.
\citet{dou20163d} introduce deep supervision by combining predictions from varying resolutions of feature maps, suggesting that it can combat potential optimization difficulties, and thus, reach a faster convergence rate and more powerful discrimination capability. \citet{zhu2017deeply} used eight additional deeply supervised layers in their proposed architecture. 
Our nested networks, however, are more amenable to training under deep supervision: 1) multiple decoders automatically generate full resolution segmentation maps; 2) the networks are embedded at various different depths of U-Net so that it grasps multi-resolution features; 3) densely connected feature maps help smooth the gradient flow and give a relatively consistent predicting mask; 4) the high dimension features have effects on all of the outputs through back-propagation, allowing us to prune the network in the inference phase.


% \subsection{Representation learning}
\subsection{Extracting Generic Image Features}
\label{ch1:related_work:extracting_generic_image_features}

With the splendid success of deep neural networks, transfer learning~\citep{pan2010survey,weiss2016survey,yosinski2014transferable} has become integral to many applications, especially medical imaging~\citep{greenspan2016guest,litjens2017survey,lu2017deep,shen2017deep,wang2017comparison,zhou2017fine,zhou2019models,zhou2021models}. This immense popularity of transfer learning is attributed to the learned image representation, which offers convergence speedups and performance gains for most target tasks, in particular, with limited annotated data. In the following sections, we review the works related to supervised and self-supervised representation learning.

\subsubsection{Supervised representation learning} 

%Annotating data at this scale required a significant research effort and budget, and was achieved using crowdsourcing platforms. It is not feasible to create annotated datasets of this size for every application.

ImageNet contains more than fourteen million annotated images that indicate which objects are present; and more than one million of the images have actually been annotated with the bounding boxes of the objects. Pre-training a model on ImageNet and then fine-tuning it on other imaging tasks has seen the most practical adoption in medical image analysis~\citep{bar2015chest,shin2016deep,tajbakhsh2016convolutional}. Despite its remarkable transferability, the 2D ImageNet model offers little benefit towards 3D medical imaging tasks in the most prominent medical modalities (\eg CT and MRI). To fit this paradigm, 3D imaging tasks have to be reformulated and solved in 2D~\citep{roth2015improving,roth2014new,tajbakhsh2015computer}, thus losing rich spatial information and inevitably compromising the performance. Annotating 3D medical images at a similar scale with ImageNet requires a significant research effort and budget. It is currently infeasible to create annotated datasets comparable to this size for every 3D medical application. 
Consequently, for lung cancer malignancy estimation, \citet{ardila2019end} resorted to incorporate spatial information by using Inflated 3D~\citep{carreira2017quo}, trained from the Kinetics dataset, as the feature extractor. Evidenced by \tablename~\ref{ch5:tab:top_existing_models}, it is a suboptimal choice due to the large domain gap between the temporal video and medical volume. This limitation has led to the development of the NiftyNet model zoo~\citep{gibson2018niftynet}. However, they were trained with small datasets for specific applications (\eg brain parcellation and organ segmentation), and were never intended as source models for transfer learning. Our experimental results, in \tablename~\ref{ch5:tab:top_existing_models}, indicate that NiftyNet models offer limited benefits to the five target medical applications via transfer learning. More recently, \citet{chen2019med3d} have pre-trained 3D residual networks by jointly segmenting the objects annotated in a collection of eight medical datasets, resulting in MedicalNet for 3D transfer learning. In \tablename~\ref{ch5:tab:top_existing_models}, we have examined the pre-trained MedicalNet on five target tasks in comparison with our Models Genesis. As reviewed, each and every aforementioned pre-trained model requires massive, high-quality annotated datasets. However, seldom do we have a perfectly-sized and systematically-annotated dataset to pre-train a deep model in medical imaging, where both data and annotation are expensive to acquire. We overcome the above limitation by using self-supervised learning, which allows models to learn image representation from abundant unannotated medical images with {\em zero} human annotation effort.

\subsubsection{Self-supervised representation learning}

Aiming at learning image representation from unannotated data, self-supervised learning research has recently experienced a surge in computer vision~\citep{caron2018deep,chen2019rotation,doersch2015unsupervised,goyal2019scaling,jing2020self,mahendran2018cross,mundhenk2018improvements,noroozi2018boosting,noroozi2016unsupervised,pathak2016context,sayed2018cross,zhang2016colorful,zhang2017split}, but it is a relatively new trend in modern medical imaging. The key challenge for self-supervised learning is identifying a suitable task that generates input and output instance pairs from the data. Two of the preliminary studies include (1) predicting the distance and 3D coordinates of two patches randomly sampled from the same brain~\citep{spitzer2018improving} and (2) identifying whether two scans belong to the same person and further predicting the level of vertebral bodies~\citep{jamaludin2017self}. Nevertheless, these two works are incapable of learning representation from ``self-supervision'' because they demand auxiliary information and specialized data collection such as paired and registered images. By utilizing only the original pixel/voxel information shipped with data, several self-supervised learning schemes have been developed for different medical applications: \citet{ross2018exploiting} adopted colorization as the proxy task, wherein color colonoscopy images are converted to gray-scale and then recovered using a conditional Generative Adversarial Network (GAN); \citet{alex2017semisupervised} pre-trained a stack of denoising auto-encoders, wherein the self-supervision was created by mapping the patches with the injected noise to the original patches; \citet{chen2019self} designed image restoration as the proxy task by first shuffling small regions of the image and then training the model to restore the original image; \citet{zhuang2019self} and \citet{zhu2020rubik} introduced a 3D representation learning proxy task by recovering the rearranged and rotated Rubik's cube; and finally \citet{tajbakhsh2019surrogate} individualized self-supervised schemes for a set of target tasks. As seen, the previously discussed self-supervised learning schemes, both in computer vision and medical imaging, are developed individually for specific target tasks; therefore, the generalizability and robustness of the learned image representation have yet to be examined across multiple target tasks. To our knowledge, we are the first to investigate cross-domain self-supervised learning in medical imaging. 